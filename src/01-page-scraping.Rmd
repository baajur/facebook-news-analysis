---
title: "Facebook Page Scraping"
author: "TJ Palanca"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output: 
  html_document:
    toc: true
    toc_float: true
    theme: "lumen"
    highlight: "tango"
    code_folding: hide
    self_contained: false
    lib_dir: libs
---

## Background

In order to be able to analyze news articles on Facebook, we need a method to extract the necessary information on posts, comments, comment replies and reactions thereto for a specific Facebook page. We use calls to the Facebook Graph API to retrieve this information.

## Preliminaries

### Libraries

We load the libraries necessary to perform the extraction:

```{r setup}

suppressPackageStartupMessages({
  library(httr)      # Making API calls
  library(jsonlite)  # JSON parsing
  library(rvest)     # Page scraping
  
  library(magrittr)  # Advanced piping
  library(purrr)     # Functional programming
  
  library(stringr)   # String manipulation
  library(dplyr)     # Data frame manipulation
  library(tibble)    # Better data frames
  library(tidyr)     # Frame shaping
  library(lubridate) # Timestamp manipulation
  
  library(htmlwidgets) # Javascript bindings
  library(DT)          # DataTables JS Library
  
  library(pushoverr)   # Push notifications upon completion of long-running processes
})

# Set proper working directory
if(interactive() & !str_detect(getwd(), "src")) setwd("src")

```

## Scraping 

### Functions

We set up some scraping functions to help use extract information from a Facebook Page.

```{r Scraping Functions}

source("../func/01-page-scraping-functions.R")

```

### Execution

In order to prevent bias in selecting the pages to be scraped, we rely on [Socialbaker's Ranking of Top 100 Media Facebook Pages in the Philippines by Total Likes](https://www.socialbakers.com/statistics/facebook/pages/total/philippines/media/page-1-5/).

```{r Top 100 Facebook Pages, cache=TRUE, message=FALSE}

map_df(
  c(
    "../dta/01-top-ph-media-fbpages-50.html",
    "../dta/02-top-ph-media-fbpages-100.html"
  ),
  function(html) {
    read_html(x = html, encoding = "UTF-8") %>% 
      html_node(".brand-table-list") %>% 
      html_table(fill = TRUE) %>% 
      slice(1:50) %>% 
      select(
        rank       = X1, 
        page_title = X2,
        page_likes = X3
      ) %>% 
      mutate(
        page_title = page_title %>% 
          str_replace_all("^.*\n|\t", ""),
        page_likes = page_likes %>% 
          str_replace_all("Total Fans|\n|\t|\\s", "") %>% 
          as.integer()
      ) %>% 
      mutate(
        page_id = 
          read_html(x = html, encoding = "UTF-8") %>% 
          html_nodes(".name .acc-placeholder-img") %>% 
          map_chr(html_attr, "href") %>% 
          str_replace_all("^.*detail/", "") %>% 
          str_extract("^[0123456789]*-") %>% 
          str_replace_all("-", "")
      ) %>% 
      mutate(
        page_name = 
          map_chr(
            page_id,
            function(page_id) {
              message(page_id)
              GET(
                paste0("https://www.facebook.com/", page_id),
                user_agent("Mozilla/5.0")
              ) %$% url %>% 
                str_extract("facebook.com/.*/$") %>% 
                str_replace_all("facebook.com/|/", "")
            }
          )
      )
  }
) -> top_100_fbpages.dt

```

```{r Display Top 100 Facebook Pages}
datatable(top_100_fbpages.dt)
```

We want to isolate the pages that are classified as news sites, which unfortunately can only be done manually.

```{r}

newspages.ls <-
  top_100_fbpages.dt$page_name[
    c(2, 4, 15, 17, 19, 22, 24, 25, 28, 30, 33, 36,
      39, 40, 44, 47, 50, 55, 69, 73, 81, 87)]

print(newspages.ls)

```

In order to start collecting information, we authenticate with a Facebook App Access Token Since page posts are public, we do not need a user access token to gather the posts and other information.

```{r Authentication}

# Load authentication files
load("../bin/fb_auth.rda")

# Get authentication
getAppAccessToken(
  fb_app_id = fb_app_id,
  fb_app_secret = fb_app_secret
)

```

We then gather all the data 

```{r Data Gathering, message = FALSE}

# Download all new

try({
  walk(
    newspages.ls[1:5],
    function(newspage) {
      # only run if file does not exist
      if (!file.exists(paste0("../dta/03-fbpage-", newspage, ".rds"))) {
        posts.ls <-
          cachedGetFBPage(
            page_name                   = newspage,
            limit_timestamp             = "2016-01-01 00:00:00",
            timezone                    = "Asia/Manila",
            cache_interval              = "1 day",
            posts_per_page              = 1,
            num_comments                = 100, 
            num_attachments             = 100,
            num_reactions               = 2000,  
            num_comments_likes          = 2000, 
            num_comments_comments       = 25,
            num_comments_comments_likes = 2000,
            access_token                = fb.tkn
          )
        
        saveRDS(object = posts.ls, file = paste0("../dta/03-fbpage-", newspage, ".rds"))
        
        rm(posts.ls)
      }
    }
  )
})

load("../bin/pushover_auth.rda")

pushover(
  title = "R Execution Completed",
  message = "adhoc03",
  priority = 1,
  user = pushover_user_key,
  app = pushover_app_key
)

```

Now that we have collected the information, we pass through each file and process it further into just a list of data frames. 

