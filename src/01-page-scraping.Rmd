---
title: "Facebook Page Scraping"
author: "TJ Palanca"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output: 
  html_document:
    toc: true
    toc_float: true
    theme: "lumen"
    highlight: "tango"
    code_folding: hide
    self_contained: false
    lib_dir: libs
---

## Background

In order to be able to analyze news articles on Facebook, we need a method to extract the necessary information on posts, comments, comment replies and reactions thereto for a specific Facebook page. We use calls to the Facebook Graph API to retreieve this information.

## Preliminaries

### Libraries

We load the libraries necessary to perform the extraction:

```{r setup}

suppressPackageStartupMessages({
  library(httr)      # Making API calls
  library(jsonlite)  # JSON parsing
  library(rvest)     # Page scraping
  
  library(magrittr)  # Advanced piping
  library(purrr)     # Functional programming
  
  library(stringr)   # String manipulation
  library(dplyr)     # Data frame manipulation
  library(tibble)    # Better data frames
  library(tidyr)     # Frame shaping
  library(lubridate) # Timestamp manipulation
  
  library(htmlwidgets)
  library(DT)
})

# Set proper working directory
if(interactive() & !str_detect(getwd(), "src")) setwd("src")

```

## Scraping 

### Functions

We set up some scraping functions to help use extract information from a Facebook Page.


```{r Scraping Functions}

convertTimeStamp <- function(ts) {
  # Converts string timestamp ts into POSIXct format
  as.POSIXct(strptime(ts, "%Y-%m-%dT%H:%M:%S%z"))
}

getAppAccessToken <- function(fb_app_id, fb_app_secret) {
  # Get app access token 
  #
  # Args:
  #   fb_app_id:      character scalar containing the Facebook App ID
  #   fb_app_secret:  character scalar containing the Fabebook App Secret
  #
  # Returns:
  #   App Access Token in fb.tkn (do not assign)
  
  GET(
    url   = "https://graph.facebook.com/oauth/access_token",
    query = list(
      client_id     = fb_app_id,
      client_secret = fb_app_secret,
      grant_type    = "client_credentials"
    )
  ) %>% 
    content() %>% 
    str_replace("^access_token=", "") ->> fb.tkn 
}

callFBGraphAPI <- function(node = "", query = NULL, version = "v2.8", 
                           access_token = fb.tkn) {
  # Function to generate a generic Facebook Graph API Call. Uses
  # API v2.8 by default.
  #
  # Args:
  #   node:         the node to be pinged
  #   query:        list of queries
  #   access_token: the access token (not needed if fb.tkn is present)
  # 
  # Returns:
  #   Unparsed resuts from the FB API call. 
  
  GET(
    url   = paste0("https://graph.facebook.com/", version, "/"),
    path  = node,
    query = append(list(access_token = access_token), query)
  ) %T>% {
    # Check for API call errors
    if(.$status_code != 200) stop (paste0("API Call Error: ", .$status_code))
  } 
  
}

expandPaging <- function(object_ids, object_data, object_next) {
  # If API results are incomplete due to paging, completes the data
  # and cleans it up into a tidy data frame
  #
  # Args:
  #   object_ids:  list of IDs to be assigned to each batch
  #   object_data: list of data frames that comprise initial data
  #   object_next: list of links returned as the next API call
  # 
  # Returns:
  #   list of bindable dataframes with complete data for all pages 
  
  # Return NULL if no reasonable input
  if (is.null(object_ids) | is.null(object_data)) return(NULL)
  
  # Replace next with NA if no next
  if (is.null(object_next)) object_next = rep(NA, length(object_ids))
  
  pmap(
    list(
      object_id   = object_ids,
      object_data = object_data,
      object_next = object_next
    ),
    function(object_id, object_data, object_next) {
      
      cat(object_id, "\n")
      
      data <- object_data
      
      while(ifelse(is.null(object_next), FALSE, 
                   ifelse(is.na(object_next), FALSE, TRUE))) {
        GET(object_next) %>% 
          content(as = "text") %>% 
          fromJSON() ->
          content
        
        if(length(content$data) > 0) {
          rbind.pages(list(data, content$data)) -> data
        }
        
        object_next <- content$paging$`next`
        
      }
      
      data$object_id <- object_id
      data
    }
  )
  
}

queryFBPosts <- function(num_comments, num_attachments, num_reactions) {
  # Generates Graph API query statement based on num
  paste0(
    "
    id, created_time, message, story, 
    attachments{
      title, description, type, target{id, url}, media{image{src}}
    },
    comments.limit(", num_comments,"){
      id, created_time, message, from{id, name},
      comments.limit(", num_comments,"){
        id, created_time, message, from{id, name},
        likes.limit(", num_reactions,")
      },
      likes.limit(", num_reactions,"){
        id, name
      },
      attachments.limit(", num_attachments,"){
        title, description, type, target{id, url}, media{image{src}}
      }
    },
    reactions.limit(", num_reactions,"){id, name, type}
    "
  )
}

getFBPage <- function(page_name, limit_posts = Inf, limit_timestamp = -Inf, 
                      posts_per_page = 5, num_comments = 50, num_attachments = 50, 
                      num_reactions = 500, access_token = fb.tkn) {
  # Take complete posts data from a Facebook page
  #
  # Args:
  #   page_name:       username or page_id of the page to be scraped
  #   limit_posts:     maximum number of posts to be scraped
  #   limit_timestamp: earliest timestamp of posts to be scraped
  #   posts_per_page:  number of posts per call (reduce if failing)
  #   access_token:    app access token (not needed if fb.tkn initialized)
  #
  # Returns:
  #   A list of post data
  
  # Raise exception if no limit is specified
  if (is.infinite(limit_posts) & is.infinite(limit_timestamp)) {
    stop("No post or date limit specified!")
  }
  
  # Raise exception if no page_name specific
  if (is.null(page_name)) stop("No page name specified!")
  
  # Make initial API call
  
  callFBGraphAPI(
    node = paste0(page_name, "/posts"),
    query = list(
      fields = queryFBPosts(num_comments, num_attachments, num_reactions),
      limit = posts_per_page
    )
  ) %>% 
    content(as = "text") %>% 
    fromJSON() -> 
    fbpage.cnt
  
  # Extract data and limit comparison data
  fbposts.ls  <- fbpage.cnt$data
  num_posts   <- nrow(fbposts.ls)
  min_created <- min(convertTimeStamp(fbposts.ls$created_time))
  cat(num_posts, "posts -", format(min_created, "%Y-%m-%d %H:%M:%S"), "\n")
  
  # Repeat while limit_posts or limit_timestamp has not been reached and 
  # next page still exists
  while (num_posts   < limit_posts & 
         min_created > limit_timestamp &
         !is.null(fbpage.cnt$paging$`next`)) {
    
    # Grab new content
    GET(fbpage.cnt$paging$`next`) %T>% {
      if (.$status_code != 200) stop(paste0("API Call Error: ", .$status_code))
    } %>% 
      content(as = "text") %>% 
      fromJSON() ->
      fbpage.cnt
    
    # Bin new content
    rbind.pages(pages = list(fbposts.ls, fbpage.cnt$data)) -> fbposts.ls
    
    # Recompute limits and report
    num_posts   <- nrow(fbposts.ls)
    min_created <- min(convertTimeStamp(fbposts.ls$created_time))
    cat(num_posts, "posts -", format(min_created, "%Y-%m-%d %H:%M:%S"), "\n")
    
  }
  
  # Limit to limit_posts if applicable
  if (!is.infinite(limit_posts)) {
    fbposts.ls[
      1:limit_posts, 
      ] -> fbposts.ls
  }
  # Limit to limit_timestamp if applicable
  if (!is.infinite(limit_timestamp)) {
    fbposts.ls[
      which(convertTimeStamp(fbposts.ls$created_time) > limit_timestamp), 
      ] -> fbposts.ls
  }
  
  # Assign page name and return
  fbposts.ls$page_name <- rep(page_name, length(fbposts.ls$id))
  fbposts.ls
  
}

tidyFBPageData <- function(posts.ls, access_token = fb.tkn) {
  # Take a list of post data and translate into tidy data frames
  # 
  # Args:
  #   posts:        list of post data generated from `getFBPage`
  #   access_token: Facebook app access token (not needed if fb.tkn exists)
  #
  # Returns:
  #   List of 7 dataframes:
  #     posts:             posts data
  #     posts_reactions:   reactions to top-level posts
  #     posts_attachments: data of articles/links/media attached to post
  #     posts_comments:    comments on posts
  #     posts_comments_likes:    likes on comments on posts
  #     posts_comments_comments: comment replies
  #     posts_comments_comments_likes: likes on comment replies
  
  # Get post data
  cat("Grabbing post data...\n")
  posts.ls %>% 
    select(-reactions, -attachments, -comments) %>% 
    mutate(
      post_timestamp_utc = convertTimeStamp(created_time)
    ) %>% 
    select(
      page_name    = page_name,
      post_id      = id, 
      post_message = message, 
      post_timestamp_utc,
      post_story   = story
    ) -> posts.dt
  
  # Get post attachments data 
  cat("Grabbing post attachments data...\n")
  posts.ls %>% 
    select(id, attachments) %>% {
      expandPaging(
        object_ids  = .$id,
        object_data = .$attachments$data,
        object_next = .$attachments$paging$`next`
      ) 
    } %>% 
    map_df(
      function(attachments) {
        data_frame(
          object_id              = attachments$object_id,
          attachment_title       = attachments$title,
          attachment_type        = attachments$type,
          attachment_target_url  = attachments$target$url,
          attachment_media_url   = ifelse(
            is.null(attachments$media$image$src), NA,
            attachments$media$image$src
          )
        )
      }
    )->
    posts_attachments.dt
  
  # Get post reactions data
  cat("Grabbing post reactions data...\n")
  posts.ls %>% 
    select(id, reactions) %>% {
      expandPaging(
        object_ids  = .$id,
        object_data = .$reactions$data,
        object_next = .$reactions$paging$`next`
      ) %>% 
        bind_rows() %>% 
        select(
          object_id     = object_id,
          reactor_id    = id,
          reactor_name  = name,
          reaction_type = type
        )
    } ->
    posts_reactions.dt
  
  # Get post comments raw data
  cat("Grabbing post comments raw data...\n")
  posts.ls %>% 
    select(id, comments) %>% {
      expandPaging(
        object_ids  = .$id,
        object_data = .$comments$data,
        object_next = .$comments$paging$`next`
      ) 
    } ->
    posts_comments.ls
  
  # Get post comments data
  cat("Grabbing post comments processed data...\n")
  posts_comments.ls %>% 
    map_df(
      function(comments) {
        if (is.null(comments$id)) return(NULL)
        data_frame(
          object_id             = comments$object_id,
          comment_id            = comments$id,
          comment_timestamp_utc = convertTimeStamp(comments$created_time),
          comment_message       = comments$message,
          commenter_name        = comments$from$name,
          commenter_id          = comments$from$id
        ) 
      }
    ) -> posts_comments.dt
  
  # Get post comments likes data
  cat("Grabbing post comments reactions data...\n")
  posts_comments.ls %>% 
    map(
      function(comments) {
        list(
          id    = comments$id,
          likes = comments$likes
        )
      }
    ) %>% 
    map(
      function(comments) {
        expandPaging(
          object_ids  = comments$id,
          object_data = comments$likes$data,
          object_next = comments$likes$paging$`next`
        ) 
      }
    ) %>% 
    flatten() %>% 
    bind_rows() %>% 
    select(
      object_id   = object_id,
      liker_id    = id,
      liker_name  = name
    ) ->
    posts_comments_likes.dt
  
  # Get post comment replies raw data
  cat("Grabbing post comments replies raw data...\n")
  posts_comments.ls %>% 
    map(
      function(comments) {
        list(
          id       = comments$id,
          comments = comments$comments
        )
      }
    ) %>% 
    map(
      function(comments) {
        expandPaging(
          object_ids  = comments$id,
          object_data = comments$comments$data,
          object_next = comments$comments$paging$`next`
        ) 
      }
    ) %>% 
    flatten() ->
    posts_comments_comments.ls
  
  # Get post comment replies data
  cat("Grabbing post comments replies processed data...\n")
  posts_comments_comments.ls %>% 
    map_df(
      function(comments) {
        if (is.null(comments$id)) return(NULL)
        data_frame(
          object_id             = comments$object_id,
          comment_id            = comments$id,
          comment_timestamp_utc = convertTimeStamp(comments$created_time),
          comment_message       = comments$message,
          commenter_name        = comments$from$name,
          commenter_id          = comments$from$id
        ) 
      }
    ) -> posts_comments_comments.dt
  
  # Get post comment replies reactions data
  cat("Grabbing post comments replies reactions data...\n")
  posts_comments_comments.ls %>% 
    map(
      function(comments) {
        list(
          id    = comments$id,
          likes = comments$likes
        )
      }
    ) %>% 
    map(
      function(comments) {
        expandPaging(
          object_ids  = comments$id,
          object_data = comments$likes$data,
          object_next = comments$likes$paging$`next`
        ) 
      }
    ) %>% 
    flatten() %>% 
    bind_rows() %>% 
    select(
      object_id   = object_id,
      liker_id    = id,
      liker_name  = name
    ) ->
    posts_comments_comments_likes.dt
  
  cat("Done!\n")
  
  list(
    posts                         = posts.dt,
    posts_attachments             = posts_attachments.dt,
    posts_reactions               = posts_reactions.dt,
    posts_comments                = posts_comments.dt,
    posts_comments_likes          = posts_comments_likes.dt,
    posts_comments_comments       = posts_comments_comments.dt,
    posts_comments_comments_likes = posts_comments_comments_likes.dt
  )
  
}


```

### Execution

In order to prevent bias in selecting the pages to be scraped, we rely on [Socialbaker's Ranking of Top 100 Media Facebook Pages in the Philippines by Total Likes](https://www.socialbakers.com/statistics/facebook/pages/total/philippines/media/page-1-5/).

```{r Top 100 Facebook Pages, cache=TRUE}

map_df(
  c(
    "../dta/01-top-ph-media-fbpages-50.html",
    "../dta/02-top-ph-media-fbpages-100.html"
  ),
  function(html) {
    read_html(x = html, encoding = "UTF-8") %>% 
      html_node(".brand-table-list") %>% 
      html_table(fill = TRUE) %>% 
      slice(1:50) %>% 
      select(
        rank       = X1, 
        page_title = X2,
        page_likes = X3
      ) %>% 
      mutate(
        page_title = page_title %>% 
          str_replace_all("^.*\n|\t", ""),
        page_likes = page_likes %>% 
          str_replace_all("Total Fans|\n|\t|\\s", "") %>% 
          as.integer()
      ) %>% 
      mutate(
        page_id = 
          read_html(x = html, encoding = "UTF-8") %>% 
          html_nodes(".name .acc-placeholder-img") %>% 
          map_chr(html_attr, "href") %>% 
          str_replace_all("^.*detail/", "") %>% 
          str_extract("^[0123456789]*-") %>% 
          str_replace_all("-", "")
      ) %>% 
      mutate(
        page_name = 
          map_chr(
            page_id,
            function(page_id) {
              GET(
                paste0("https://www.facebook.com/", page_id),
                user_agent("Mozilla/5.0")
              ) %$% url %>% 
                str_extract("facebook.com/.*/$") %>% 
                str_replace_all("facebook.com/|/", "")
            }
          )
      )
  }
) -> top_100_fbpages.dt

```

```{r}
datatable(top_100_fbpages.dt)
```

We want to isolate the pages that are classified as news sites, which unfortunately have to be manually done.

```{r}

newspages.ls <-
  top_100_fbpages.dt$page_name[
    c(2, 4, 15, 17, 19, 22, 24, 25, 28, 30, 33, 36,
      39, 40, 44, 47, 50, 55, 69, 73, 81, 87)]

print(newspages.ls)

```

```{r}

# Load authentication files
load("../bin/fb_auth.rda")

# Get authentication
getAppAccessToken(
  fb_app_id = fb_app_id,
  fb_app_secret = fb_app_secret
)

# Loop through news pages and grab content for each
map(
  c("rapplerdotcom"),
  function(page_name) {
    getFBPage(
      page_name       = page_name,
      limit_timestamp = as.Date('2016-01-01'), # All posts from January 1, 2016
      posts_per_page  = 10,
      num_comments    = 10,
      num_reactions   = 100,
      access_token    = fb.tkn
    ) %>% 
      tidyFBPageData(access_token = fb.tkn)
  }
) -> newspages_content.ls

```

