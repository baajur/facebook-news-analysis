---
title:  "Facebook Page Scraping"
author: "Troy James Palanca // mail@tjpalanca.com"
date:   "`r format(Sys.time(), '%d %B, %Y')`"
output: html_document
---

## Background

In order to be able to analyze news articles on Facebook, we need a method to extract the necessary information on posts, comments, comment replies and reactions thereto for a specific Facebook page. We use calls to the Facebook Graph API to retrieve this information.

## Preliminaries

### Libraries

We load the libraries necessary to perform the extraction:

```{r}

suppressPackageStartupMessages({
  library(httr)      # Making API calls
  library(jsonlite)  # JSON parsing
  library(rvest)     # Page scraping
  
  library(magrittr)  # Advanced piping
  library(purrr)     # Functional programming
  
  library(stringr)   # String manipulation
  library(dplyr)     # Data frame manipulation
  library(tibble)    # Better data frames
  library(tidyr)     # Frame shaping
  library(lubridate) # Timestamp manipulation
  
  library(htmlwidgets) # Javascript bindings
  library(DT)          # DataTables JS Library
  
  library(pushoverr)   # Push notifications
})

# Set proper working directory
if(interactive() & !str_detect(getwd(), "src")) setwd("src")

# Load Pushover credentials
load("../bin/pushover_auth.rda")

pushNotification <- function(text = "R Execution Completed") {
  pushover(
    title = text,
    url = "http://adhoc01.tjpalanca.com",
    message = "adhoc01.tjpalanca.com",
    user = pushover_user_key,
    app = pushover_app_key
  )
}

```

## Scraping 

### Functions

We set up some scraping functions to help use extract information from a Facebook Page.

```{r Scraping Functions}

source("../func/01-page-scraping-functions.R")

```

### Execution

#### Top 100 Facebook Pages

In order to prevent bias in selecting the pages to be scraped, we rely on [Socialbaker's Ranking of Top 100 Media Facebook Pages in the Philippines by Total Likes](https://www.socialbakers.com/statistics/facebook/pages/total/philippines/media/page-1-5/).

```{r Top 100 Facebook Pages, cache=TRUE, message=FALSE}

map_df(
  c(
    "../dta/01-top-ph-media-fbpages-50.html",
    "../dta/02-top-ph-media-fbpages-100.html"
  ),
  function(html) {
    read_html(x = html, encoding = "UTF-8") %>% 
      html_node(".brand-table-list") %>% 
      html_table(fill = TRUE) %>% 
      slice(1:50) %>% 
      select(
        rank       = X1, 
        page_title = X2,
        page_likes = X3
      ) %>% 
      mutate(
        page_title = page_title %>% 
          str_replace_all("^.*\n|\t", ""),
        page_likes = page_likes %>% 
          str_replace_all("Total Fans|\n|\t|\\s", "") %>% 
          as.integer()
      ) %>% 
      mutate(
        page_id = 
          read_html(x = html, encoding = "UTF-8") %>% 
          html_nodes(".name .acc-placeholder-img") %>% 
          map_chr(html_attr, "href") %>% 
          str_replace_all("^.*detail/", "") %>% 
          str_extract("^[0123456789]*-") %>% 
          str_replace_all("-", "")
      ) %>% 
      mutate(
        page_name = 
          map_chr(
            page_id,
            function(page_id) {
              message(page_id)
              GET(
                paste0("https://www.facebook.com/", page_id),
                user_agent("Mozilla/5.0")
              ) %$% url %>% 
                str_extract("facebook.com/.*/$") %>% 
                str_replace_all("facebook.com/|/", "")
            }
          )
      )
  }
) -> top_100_fbpages.dt

```

```{r Display Top 100 Facebook Pages}
datatable(top_100_fbpages.dt)
```

We want to isolate the pages that are classified as news sites, which unfortunately can only be done manually.

```{r}

newspages.ls <-
  top_100_fbpages.dt$page_name[
    c(2, 4, 15, 17, 19, 22, 24, 25, 28, 30, 33, 36,
      39, 40, 44, 47, 50, 55, 69, 73, 81, 87)]

print(newspages.ls)

```

#### Scraping

In order to start collecting information, we authenticate with a Facebook App Access Token Since page posts are public, we do not need a user access token to gather the posts and other information.

```{r Authentication}

# Load authentication files
load("../bin/fb_auth.rda")

# Get authentication
getAppAccessToken(
  fb_app_id = fb_app_id,
  fb_app_secret = fb_app_secret
)

```

We then gather all the data.

```{r Data Gathering, message = FALSE}

# Download all new

try({
  walk(
    newspages.ls,
    function(newspage) {
      # only run if file does not exist
      if (!file.exists(paste0("../dta/03-fbpage-", newspage, ".rds")) &
          !file.exists(paste0("../dta/03-fbpageDONE-", newspage, ".rds"))) {
        posts.ls <-
          cachedGetFBPage(
            page_name                   = newspage,
            limit_timestamp             = "2016-01-01 00:00:00",
            timezone                    = "Asia/Manila",
            cache_interval              = "1 day",
            posts_per_page              = 5,
            num_comments                = 100, 
            num_attachments             = 100,
            num_reactions               = 2000,  
            num_comments_likes          = 2000, 
            num_comments_comments       = 25,
            num_comments_comments_likes = 2000,
            access_token                = fb.tkn
          )
        
        saveRDS(object = posts.ls, 
                file = paste0("../dta/03-fbpage-", newspage, ".rds"))
        
        rm(posts.ls)
      }
    }
  )
})

pushNotification()

```

#### Storage

Now that we have collected the information, we create a SQLite database so we can process it outside of memory.

We then parse through each of the data files and load them into memory.

```{r Database Storage}

# Get all cache files
fbpage_data.ls <-
  list.files("../dta", full.names = TRUE) %>% {
    .[str_detect(., "/03-fbpage-.*\\.rds")]
  }

# Initialize variables
posts.dt                         <- data_frame()
posts_attachments.dt             <- data_frame()
posts_reactions.dt               <- data_frame()
posts_comments.dt                <- data_frame()
posts_comments_likes.dt          <- data_frame()
posts_comments_comments.dt       <- data_frame()
posts_comments_comments_likes.dt <- data_frame()

# Walk through each cache file
walk(
  fbpage_data.ls,
  function(data_path) {
    # Read in data
    pushNotification(paste0("Processing ", data_path, "..."))
    message("Processing ", data_path, "...")
    data <- readRDS(data_path)
    
    # Combine data frames in each shard
    walk(
      data,
      function(shard) {
        if (is.data.frame(shard$posts)) {
          posts.dt %>%
            bind_rows(shard$posts) ->>
            posts.dt
        }
        if (is.data.frame(shard$posts_attachments)) {
          posts_attachments.dt %>%
            bind_rows(shard$posts_attachments) ->>
            posts_attachments.dt
        }
        if (is.data.frame(shard$posts_reactions)) {
          posts_reactions.dt %>%
            bind_rows(shard$posts_reactions) ->>
            posts_reactions.dt
        }
        if (is.data.frame(shard$posts_comments)) {
          posts_comments.dt %>%
            bind_rows(shard$posts_comments) ->>
            posts_comments.dt
        }
        if (is.data.frame(shard$posts_comments_likes)) {
          posts_comments_likes.dt %>%
            bind_rows(shard$posts_comments_likes) ->>
            posts_comments_likes.dt
        }
        if (is.data.frame(shard$posts_comments_comments)) {
          posts_comments_comments.dt %>%
            bind_rows(shard$posts_comments_comments) ->>
            posts_comments_comments.dt
        }
        if (is.data.frame(shard$posts_comments_comments_likes)) {
          posts_comments_comments_likes.dt %>%
            bind_rows(shard$posts_comments_comments_likes) ->>
            posts_comments_comments_likes.dt
        }
      }
    )

    # Remove data from memory
    rm(data)
    message("Done processing ", data_path, "!")
  }
)
  
```

