---
title: "Facebook Topic Modeling"
subtitle: "tjpalanca.com"
author: "TJ Palanca"
date: "4 Feb 2017"
output: 
  html_document:
    toc: true
    toc_float: true
    theme: "lumen"
    highlight: "tango"
    code_folding: hide
---

## Background

### Motivation

A common subject of debate in how media relays information to the public is the relative coverage that a particular topic receives in their Facebook feeds. We attempt to objectively answer this question by using topic modelling techniques.

### Methodology

In order to determine the distribution of topics that news websites cover, we need to find a way to map the the headline, caption, and other raw text to a particular topic, without having prior knowledge on what those topics are. This translates to an unsupervised classification problem on natural language. One of the most common algorithms for dealing with this information is **Latent Dirichlet Allocation (LDA).**^[[Blei, Ng, & Jordan (2003). Latent Dirichlet Allocation. *Journal of Machine Learning Research 3 (2003) 993-1022*.](http://jmlr.csail.mit.edu/papers/v3/blei03a.html)]

This model views the text generation process as conforming to the following characteristics: 

  * **A topic is a mixture of non-exclusive words.** A topic is comprised of many words, and each word maps to one or more topics.
  * **A document (in this case, a news post), is a mixture of topics.** Each document can be thought of as containing a proportion of words from each topic.

LDA is the algorithm that simultaneously maps words to topics and topics to documents. One of the important considerations of this model is that the number of topics $k$ must be known a-priori.

See [here](https://www.youtube.com/watch?v=3mHy4OSyRf0) for an excellent detailed video explanation of the LDA algorithm, [here](http://www.jmlr.org/papers/volume3/blei03a/blei03a.pdf) for the seminal paper on the algorithm, and [here](https://cran.r-project.org/web/packages/topicmodels/vignettes/topicmodels.pdf) for the specific implementation that I am implementing in R.

## Preliminaries

We load libraries needed for this analysis.

```{r}

suppressPackageStartupMessages({
  library(dplyr)     # Data manipulation
  library(stringr)   # String manipulation
  library(lubridate) # Date and time manipulation
  library(purrr)     # Functional programming
  library(tidyr)     # Reshaping
  library(magrittr)  # Advanced piping
  
  library(ggplot2)   # Static data visualization
  
  library(httr)      # HTTP functions
  library(jsonlite)  # JSON parsing
  
  library(tidytext)  # Tidy text mining
})

# Set proper working directory
if(interactive() & !str_detect(getwd(), "src")) setwd("src")

```

## Data Import

From the data that we have extracted via the Facebook Graph API (details can be found [here](http://www.tjpalanca.com/static/20170208-fb-scraping.html)).

```{r Database Pointer Definition}

# Create linkage to database
src_sqlite(
  path   = "../dta/03-fbpages.sqlite",
  create = FALSE
) -> fbpages.sqlite

# Define required tables
fbpages.sqlite %>% tbl("fact_posts") -> posts.sql
fbpages.sqlite %>% tbl("fact_posts_attachments") -> attachments.sql

```

```{r Extract data}

posts.sql %>% 
  left_join(attachments.sql, by = c("post_id" = "object_id")) %>% 
  collect(n = Inf) ->
  posts.dt

```

```{r Data cleaning}

posts.dt %>% 
  # Remove duplicates
  group_by(post_id) %>% 
  filter(row_number() == 1) %>% 
  ungroup() %>% 
  # Timestamp type and timezone conversion
  mutate(
    post_timestamp_utc = 
      post_timestamp_utc %>%
      as.POSIXct(origin = "1970-01-01", tz = "UTC"),
    post_timestamp_local = 
      post_timestamp_utc %>% 
      with_tz("Asia/Manila")
  ) ->
  posts_clean.dt

```

## Data Augmentation

We require additional fields in order to make the analysis richer. We query from the Facebook Graph API for the `description`, `url`, and `attachment_media_src` field of the attachments.

```{r Data augmentation}

if (!file.exists("../cache/topic-modeling-posts-augment.rds")) {
  # Source original scraping functions
  source("../func/01-page-scraping-functions.R")
  
  # Authenticate
  load("../bin/fb_auth.rda")
  
  getAppAccessToken(
    fb_app_id = fb_app_id,
    fb_app_secret = fb_app_secret
  )
  
  # Create augmentation function
  getFBAttachmentData <- function(post_ids, partition) {
    cat(unique(partition), "\n")
    Sys.sleep(0.5)
    callFBGraphAPI(
      node  = "attachments",
      query = list(
        ids = paste0(post_ids, collapse = ","),
        fields = "description,url,media{image{src}}"
      )
    )
  }
  
  # Loop over attachments and prepare data
  posts_clean.dt %>% 
    mutate(partition = ceiling(row_number()/20)) %>% 
    split(.$partition) %>% 
    map(~getFBAttachmentData(.$post_id, .$partition)) ->
    posts_augment.ls
  
  # Flatten and process data
  posts_augment.ls %>% 
    map(~map(., "data")) %>% 
    flatten_df("post_id") %>% 
    mutate(media = map_chr(media, ~.$src %||% NA_character_)) %>% 
    rename(
      attachment_description = description, 
      attachment_url         = url,
      attachment_media_src   = media
    ) -> 
    posts_augment.dt
  
  # Save to cache
  saveRDS(posts_augment.dt, "../cache/topic-modeling-posts-augment.rds")
} else {
  readRDS("../cache/topic-modeling-posts-augment.rds") ->
    posts_augment.dt
}

```

Now that all data transforms are complete, we consolidate objects in memory.

```{r Consolidation}

# Join to original table
posts_clean.dt %>% 
  left_join(posts_augment.dt, by = "post_id") %>% 
  select(-attachment_media_url) ->
  posts_complete.dt

# Filter to 2016 posts and rename to original file
posts_complete.dt %>% 
  filter(year(post_timestamp_local) == 2016) -> 
  posts.dt

# Remove objects in memory
rm(posts_augment.dt, posts_clean.dt, posts_complete.dt, posts_augment.ls)

```

## Pre-processing

### Shaping

For this analysis, we only include post types that do easily lend themselves to text analysis, therefore video, photo, and other complex post types are removed; only types `share` and `quoted_share` are included.

In this dataset, we have three columns of text data on which to perform our analysis:

  * `post_message` - the message that appears on the top of the article and is written for each post,
  * `attachment_title` - the title of the linked article, usually the headline, and 
  * `attachment_description` - the summary text appearing below the title, usually an excerpt of the full article linked.
  
We took a look at possible discrepancies in the data by looking at cases where any of these columns are empty - we have isolated them as either data errors or deliberately empty. In either case, they do not warrant special treatment.

We simple construct the raw text field by concatenating the three columns, separated by a space. We then tokenize it by word to transform into tidy format.

```{r Data shaping}

posts.dt %>% 
  # Filter to text types only
  filter(attachment_type %in% c("share", "quoted_share")) %>% 
  # Construct text field 
  unite(text, post_message, attachment_title, attachment_description, sep = " ") %>% 
  # Retain only relevant columns 
  select(post_id, text) %>% 
  # Tokenize by word
  unnest_tokens(word, text, token = "words", to_lower = TRUE) ->
  # Assign to variable
  posts_tokenized.dt

```

### Spell checking


### Stemming

Stemming is process of converting words into their root words, so we get to the root idea of the issue and relate similar words to each other.

### Invalid characters

We first try to filter out words with invalid characters.

```{r Show unique characters}

# Get unique characters
posts_tokenized.dt %>% 
  mutate(chars = str_split(word, pattern = "")) ->
  flatten_chr(word) %>% 
  unique() -> 
  unique_characters.vec

unique_characters.vec

```

As you can see there are quite a few characters that don't make sense.

```{r}

# Get invalid characters
unique_characters.vec[!str_detect(unique_characters.vec, "[a-z'’ñ]")] ->
  invalid_characters.vec

paste0("[", paste0(invalid_characters.vec, collapse = ""), "]") -> 
  invalid_characters.rgx

# View words with invalid characters 
posts_tokenized.dt %>% 
  distinct(word) %>% 
  filter(str_detect(word, invalid_characters.rgx)) %>% 
  filter(!str_detect(word, "[0-9]")) ->
  a

View(a)

```

### Stop Words

We first eliminate stopwords, i.e. words that are common to a language and do not contain meaning. For the purpose of this model, we use stopwords for English and Filipino/Tagalog.

```{r Download stopwords}

```

### Uninformative words

## Analysis

### Latent Dirichlet Allocation

### Topic Distributions by Page

### Topic Distributions over Time
