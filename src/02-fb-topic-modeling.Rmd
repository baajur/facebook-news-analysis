---
title: "Facebook Topic Modeling"
subtitle: "tjpalanca.com"
author: "TJ Palanca"
date: "4 Feb 2017"
output: 
  html_document:
    toc: true
    toc_float: true
    theme: "lumen"
    highlight: "tango"
    code_folding: hide
---

## Background

### Motivation

A common subject of debate in how media relays information to the public is the relative coverage that a particular topic receives in their Facebook feeds. We attempt to objectively answer this question by using topic modelling techniques.

### Methodology

In order to determine the distribution of topics that news websites cover, we need to find a way to map the the headline, caption, and other raw text to a particular topic, without having prior knowledge on what those topics are. This translates to an unsupervised classification problem on natural language. One of the most common algorithms for dealing with this information is **Latent Dirichlet Allocation (LDA).**^[[Blei, Ng, & Jordan (2003). Latent Dirichlet Allocation. *Journal of Machine Learning Research 3 (2003) 993-1022*.](http://jmlr.csail.mit.edu/papers/v3/blei03a.html)]

This model views the text generation process as conforming to the following characteristics: 

  * **A topic is a mixture of non-exclusive words.** A topic is comprised of many words, and each word maps to one or more topics.
  * **A document (in this case, a news post), is a mixture of topics.** Each document can be thought of as containing a proportion of words from each topic.

LDA is the algorithm that simultaneously maps words to topics and topics to documents. One of the important considerations of this model is that the number of topics $k$ must be known a-priori.

See [here](https://www.youtube.com/watch?v=3mHy4OSyRf0) for an excellent detailed video explanation of the LDA algorithm, [here](http://www.jmlr.org/papers/volume3/blei03a/blei03a.pdf) for the seminal paper on the algorithm, and [here](https://cran.r-project.org/web/packages/topicmodels/vignettes/topicmodels.pdf) for the specific implementation that I am implementing in R.

## Preliminaries

We load libraries needed for this analysis.

```{r Setup}

suppressPackageStartupMessages({
  library(dplyr)       # Data manipulation
  library(stringr)     # String manipulation
  library(lubridate)   # Date and time manipulation
  library(purrr)       # Functional programming
  library(tidyr)       # Reshaping
  library(magrittr)    # Advanced piping
  
  library(pushoverr)   # Pushover notifications
  
  library(doMC)        # Parallel Computin
  
  library(ggplot2)     # Static data visualization
  
  library(httr)        # HTTP functions
  library(jsonlite)    # JSON parsing
  
  library(tidytext)    # Tidy text mining
  library(hunspell)    # Text processing
  library(stringdist)  # String distances
  library(topicmodels) # Topic modelling
})

# Set proper working directory
if(interactive() & !str_detect(getwd(), "src")) setwd("src")

# Theming
quartzFonts(
  Roboto = 
    c("RobotoCondensed-Light",
      "RobotoCondensed-Bold",
      "RobotoCondensed-Italic",
      "RobotoCondensed-BoldItalic")
)

theme_set(
  theme_bw(base_family = "Roboto", base_size = 14) +
    theme(
      plot.title = element_text(face = "bold")
    )
)

# Functions
source("../func/01-page-scraping-functions.R")
source("../func/02-topic-modelling-functions.R")

```

## Data Import

From the data that we have extracted via the Facebook Graph API (details can be found [here](http://www.tjpalanca.com/static/20170208-fb-scraping.html)).

```{r Database Pointer Definition}

# Create linkage to database
src_sqlite(
  path   = "../dta/03-fbpages.sqlite",
  create = FALSE
) -> fbpages.sqlite

# Define required tables
fbpages.sqlite %>% tbl("fact_posts") -> posts.sql
fbpages.sqlite %>% tbl("fact_posts_attachments") -> attachments.sql

```

```{r Extract data}

posts.sql %>% 
  left_join(attachments.sql, by = c("post_id" = "object_id")) %>% 
  collect(n = Inf) ->
  posts.dt

```

```{r Data cleaning}

posts.dt %>% 
  # Remove duplicates
  group_by(post_id) %>% 
  filter(row_number() == 1) %>% 
  ungroup() %>% 
  # Timestamp type and timezone conversion
  mutate(
    post_timestamp_utc = 
      post_timestamp_utc %>%
      as.POSIXct(origin = "1970-01-01", tz = "UTC"),
    post_timestamp_local = 
      post_timestamp_utc %>% 
      with_tz("Asia/Manila")
  ) ->
  posts_clean.dt

```

## Data Augmentation

We require additional fields in order to make the analysis richer. We query from the Facebook Graph API for the `description`, `url`, and `attachment_media_src` field of the attachments.

```{r Data augmentation}

if (!file.exists("../cache/topic-modeling-posts-augment.rds")) {
  
  # Authenticate
  load("../bin/fb_auth.rda")
  
  getAppAccessToken(
    fb_app_id = fb_app_id,
    fb_app_secret = fb_app_secret
  )
  
  # Create augmentation function
  getFBAttachmentData <- function(post_ids, partition) {
    cat(unique(partition), "\n")
    Sys.sleep(0.5)
    callFBGraphAPI(
      node  = "attachments",
      query = list(
        ids = paste0(post_ids, collapse = ","),
        fields = "description,url,media{image{src}}"
      )
    )
  }
  
  # Loop over attachments and prepare data
  posts_clean.dt %>% 
    mutate(partition = ceiling(row_number()/20)) %>% 
    split(.$partition) %>% 
    map(~getFBAttachmentData(.$post_id, .$partition)) ->
    posts_augment.ls
  
  # Flatten and process data
  posts_augment.ls %>% 
    map(~map(., "data")) %>% 
    flatten_df("post_id") %>% 
    mutate(media = map_chr(media, ~.$src %||% NA_character_)) %>% 
    rename(
      attachment_description = description, 
      attachment_url         = url,
      attachment_media_src   = media
    ) -> 
    posts_augment.dt
  
  # Save to cache
  saveRDS(posts_augment.dt, "../cache/topic-modeling-posts-augment.rds")
} else {
  readRDS("../cache/topic-modeling-posts-augment.rds") ->
    posts_augment.dt
}

```

Now that all data transforms are complete, we consolidate objects in memory.

```{r Consolidation}

# Join to original table
posts_clean.dt %>% 
  left_join(posts_augment.dt, by = "post_id") %>% 
  select(-attachment_media_url) ->
  posts_complete.dt

# Filter to 2016 posts and rename to original file
posts_complete.dt %>% 
  filter(year(post_timestamp_local) == 2016) -> 
  posts.dt

# Remove objects in memory
rm(posts_augment.dt, posts_clean.dt, posts_complete.dt, posts_augment.ls)

```

## Pre-processing

We use a Tagalog hunspell library created by Jan Alonzo (GNU GPLv2) for the succeeding parts of this analysis.

### Shaping

For this analysis, we only include post types that do easily lend themselves to text analysis, therefore video, photo, and other complex post types are removed; only types `share` and `quoted_share` are included.

In this dataset, we have three columns of text data on which to perform our analysis:

  * `post_message` - the message that appears on the top of the article and is written for each post,
  * `attachment_title` - the title of the linked article, usually the headline, and 
  * `attachment_description` - the summary text appearing below the title, usually an excerpt of the full article linked.
  
We took a look at possible discrepancies in the data by looking at cases where any of these columns are empty - we have isolated them as either data errors or deliberately empty. In either case, they do not warrant special treatment.

We simple construct the raw text field by concatenating the three columns, separated by a space. We then tokenize it by word to transform into tidy format.

```{r Data shaping}

posts.dt %>% 
  # Filter to text types only
  filter(attachment_type %in% c("share", "quoted_share")) %>% 
  # Construct text field 
  unite(text, post_message, attachment_title, attachment_description, sep = " ") %>% 
  # Retain only relevant columns 
  select(post_id, text) %>% 
  # Tokenize by word
  unnest_tokens(word, text, token = "words", to_lower = TRUE) %>%
  unnest_tokens(word, word, token = "regex", pattern = "[.]", collapse = FALSE) %>% 
  unnest_tokens(word, word, token = "regex", pattern = "[_]", collapse = FALSE) %>% 
  unnest_tokens(word, word, token = "regex", pattern = "[:]", collapse = FALSE) %>% 
  # Cleaning apostrophe words 
  mutate(word = word %>% str_replace("['’‘].*$", "")) ->
  # Assign to variable
  posts_tokenized.dt

```

### Invalid characters and words

We first try to filter out words with invalid characters and numbers.

```{r Show unique characters}

# Get unique characters
posts_tokenized.dt %>% 
  distinct(word) %>% 
  mutate(word = iconv(word, to = "UTF-8")) %>% 
  mutate(chars = str_split(word, pattern = "")) %$%
  flatten_chr(chars) %>% 
  unique() -> 
  unique_characters.vec

unique_characters.vec

```

As you can see there are quite a few characters that don't make sense. We remove words that contain invalid characters.

```{r Remove words with invalid characters}

# Get invalid characters
unique_characters.vec[!str_detect(unique_characters.vec, "[a-zá-ž]")] %>%
  { paste0("[", paste0(., collapse = ""), "]") } -> 
  invalid_characters.rgx

# View words with invalid characters 
posts_tokenized.dt %>% 
  filter(!str_detect(word, invalid_characters.rgx)) ->
  posts_tokenized.dt

```

We also remove words that are less than 3 characters long. 

```{r Remove short words}

# Remove short words
posts_tokenized.dt %>% 
  filter(str_length(word) >= 3) ->
  posts_tokenized.dt

```

### Spell checking

Given that this is post information and not comments, we don't have to do much spell checking, but we do so as a precautionary step.

```{r Sample spell checking}

posts_tokenized.dt %>% 
  # Tagalog and English spell checks
  filter(!hunspell_check(word, "tl_PH") & !hunspell_check(word, "en_US")) %>%
  distinct(word) -> 
  posts_misspelled_words.dt

posts_misspelled_words.dt %>% sample_n(1000) %$% word
  
```

It seems that most of the words that do not pass the spell check are either not in the dictionary or are proper nouns. We therefore decide that it's not worth correcting the spelling as it will probably impact the true meaning of the words.

### Stemming

Stemming is process of converting words into their root words, so we get to the root idea of the issue and relate similar words to each other. Stemming implemented by `hunspell` and `SnowballC` seems to be too strict, so we implement the following stemming patterns (regex):

  * English: "es$", "s$", "ed$", "d$", "ing$", "ly$"
  * Tagalog: "^nag", "^na", "um", "in", "an$", "in$"

```{r Stem word samples}

# Compute stem words
posts_tokenized.dt %>% 
  stemWords(c("es$", "s$", "ed$", "d$", "ing$", "ly$"), "en_US") ->
  posts_stems_en.dt

posts_tokenized.dt %>% 
  stemWords(c("^nag", "^na", "um", "in", "an$", "in$"), "tl_PH") ->
  posts_stems_tl.dt

# Show some examples
posts_stems_en.dt %>% sample_n(20)
posts_stems_tl.dt %>% sample_n(20)

```

```{r Stemming}

# Check no overlap between tagalog and english stem words
stopifnot(
  intersect(posts_stems_tl.dt$word, posts_stems_en.dt$word) %>% length() == 0
)

# Combine
posts_stems.dt <- 
  rbind(
    posts_stems_en.dt,
    posts_stems_tl.dt
  )

# Add to tokenized posts
posts_tokenized.dt %>% 
  left_join(posts_stems.dt, by = "word") %>% 
  mutate(word = ifelse(is.na(stem_word), word, stem_word)) %>% 
  select(-stem_word) -> 
  posts_tokenized.dt

# Clean up
rm(posts_stems.dt, posts_stems_en.dt, posts_stems_tl.dt)

```

### Stop Words

We first eliminate stopwords, i.e. words that are common to a language and do not contain meaning. For the purpose of this model, we use stopwords for English and Filipino/Tagalog.

```{r Stop word removal}

# Collect English and Tagalog stopwords
stopwords.ls <- c(
  stop_words$word, 
  fromJSON(
    "https://raw.githubusercontent.com/stopwords-iso/stopwords-tl/master/stopwords-tl.json"
  ),
  # Manual stopwords
  "thy", "monday", "tuesday", "wednesday", "thursday", "friday", "saturday", "sunday",
  "befullyinformed", "sen", "cnn", "final", "start", "san", "anti", "http", "https",
  "iii"
)

# Filter out stopwords
posts_tokenized.dt %>% 
  filter(!word %in% stopwords.ls) -> 
  posts_tokenized.dt

```

### Filtering and Casting

We summarise the data format into a more compressed form and convert it into a document term matrix.

```{r Term Frequency}

# Summarise
posts_tokenized.dt %>% 
  group_by(post_id, word) %>% 
  summarise(term_frequency = n()) %>% 
  ungroup() ->
  posts_tokenized.dt

```

We remove words that do not appear in at least 5 documents (3 posts are totally eliminated).

```{r Remove rare words}

# Select rare words
posts_tokenized.dt %>% 
  group_by(word) %>% 
  summarise(count = n()) %>% 
  ungroup() %>% 
  filter(count < 5) ->
  posts_rare_words.dt

# Remove rare words
posts_tokenized.dt %>% 
  anti_join(posts_rare_words.dt, by = "word") ->
  posts_tokenized.dt

# Clean up
rm(posts_rare_words.dt)

```

```{r Cast to DTM}

# Convert to document term matrix
posts_tokenized.dt %>% 
  cast_dtm(post_id, word, term_frequency) ->
  posts_tokenized.dtm

```

## Analysis

### Topic modelling

#### Setting the number of topics

In order to determine the ideal number of topics $k$, we perform 5-fold cross validation on [perplexity](https://en.wikipedia.org/wiki/Perplexity) at different values of $k$. We then compute the rate of perplexity change (RPC) For more information on this method of selection, see [Zhao et al. (2015)](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4597325/).

On a 10% random sample, we estimate the perplexity using 5-fold cross validation for different values of $k$.

```{r Cross validating k}

set.seed(7292)
crossValidatePerplexity(
  dtm          = posts_tokenized.dtm[
    sample(1:posts_tokenized.dtm$nrow, posts_tokenized.dtm$nrow * 0.1),],
  n_folds      = 5,
  k_candidates = c(10, 15, 20, 25, 30, 35, 40, 45, 50),
  method       = "Gibbs"
) -> 
  posts_tokenized_perplexity_cv.dt

```

```{r}

plotCrossValidatedPerplexity(posts_tokenized_perplexity_cv.dt) +
  annotate("segment", x = 40, xend = 40, y = -Inf, yend = Inf, color = "red") +
  labs(
    title = "Rate of Perplexity Change",
    subtitle = 
      "LDA Perplexity, 5-fold cross validation, 2016 Facebook News Corpus",
    x = "Number of Topics (k)",
    y = "Rate of Perplexity Change\n(units/marginal topic)",
    caption = "\nLine indicates average perplexity\nPoints indicate folds"
  )  

```

*Note: this took 8 hours to run in parallel on 3 cores.*

As you can see, the change point (where the rate of perplexity change no longer falls significantly with additional topics) is at 40 topics.

### LDA Training

We then train the LDA on the full dataset, with $k = 40$.

```{r LDA training}

# Train the LDA model
LDA(
  x       = posts_tokenized.dtm,
  k       = 40,
  method  = "Gibbs",
  control = list(seed = 7292)
  
) -> posts.lda
          
pushNotification("LDA Training Complete!")

```

Training took 2 hours on the full dataset.

### Topic Naming

